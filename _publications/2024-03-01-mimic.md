---
title: "Weakly-Supervised Multimodal Learning on MIMIC-CXR"
collection: publications
permalink: /publication/2024-03-01-mimic
excerpt: 'This paper is about MM-VAMP VAE, a multimodal variational autoencoder that allows latent representation sharing, obtained by a mixture-of-experts prior with a soft constraint, inspired by the Jensen-Shannon Divergence in contrastive learning, that is great at downstream tasks in representation learning and missing modality imputation, in both simulation study and a real world neuron activity datasets.'
date: 2024-03-08
venue: 'Machine Learning for Health (ML4H) symposium 2024'
paperurl: 'https://arxiv.org/pdf/2411.10356'
citation: 'Agostini A., Chopard D., Meng Y., Fortin N., Shahbaba B., Mandt S., Sutter T. M., Vogt J. E. (2024). &quot;Weakly-Supervised Multimodal Learning on MIMIC-CXR&quot; <i>arXiv Preprint</i> arXiv: 2411.10356, 2024.'
---

Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in-depth evaluation of the newly proposed Multimodal Variational Mixture-of Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our analysis demonstrates that the MMVM VAE consistently outperforms other multimodal VAEs and fully supervised approaches, highlighting its strong potential for real-world medical applications.

Keywords: Multimodal Learning; Representation Learning, Multimodal Learning, Medical Imaging Analysis, Chest X-rays
