---
title: "Unity by Diversity: Improved Representation Learning in Multimodal VAEs"
collection: publications
permalink: /publication/2024-03-08-mmvm
excerpt: 'This paper is about MM-VAMP VAE, a multimodal variational autoencoder that allows latent representation sharing, obtained by a mixture-of-experts prior with a soft constraint, inspired by the Jensen-Shannon Divergence in contrastive learning, that is great at downstream tasks in representation learning and missing modality imputation, in both simulation study and a real world neuron activity datasets.'
date: 2024-03-08
venue: 'Advances in Neural Information Processing Systems 38 (NeurIPS 2024)'
paperurl: 'https://arxiv.org/pdf/2403.05300.pdf'
citation: 'Sutter T. M., Meng Y., Agostini A., Chopard D., Fortin N., Vogt J. E., Shahbaba B., Mandt S. (2024). &quot;Unity by Diversity: Improved Representation Learning in Multimodal VAEs&quot; <i>arXiv Preprint</i> arXiv: 2403.05300, 2024.'
---

Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modalityâ€™s latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of missing data modalities compared to existing methods.

Keywords: Multimodal Learning; Representation Learning; Variational Autoencoders; Contrastive Learning; Vamp Prior; Text-to-image Generation; Neuroscience
